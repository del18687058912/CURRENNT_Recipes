Started in hybrid online/batch training mode.
Mini-batches (1 sequences each) will be shuffled during training.
Using input noise with a standard deviation of 0.1.
The trained network will be written to 'trained_network.jsn'.
Validation error will be calculated every 1 epochs.
Training will be stopped after 40 epochs or if there is no new lowest validation error within 5 epochs.
Autosave after EVERY EPOCH enabled.
Utilizing the GPU for computations with 1 sequences in parallel.
Uniform distribution with layer-wise range
. Random seed: 2012961635

Using device #0 (Tesla K40c)
Reading network from './network.jsn'... done.

Loading training set '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc1' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc2' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc3' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc4' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc5' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc6' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc7' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc8' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc9' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc10' '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc11train' ...
using cache file: /tmp/3551-108d-f1b8-e897
... done.
Loaded fraction:  100%
Sequences:        11572
Sequence lengths: 140..4034
Total timesteps:  11544584

Loading validation set '/work/smg/wang/PROJ/WE/DNNAM/DATA/nancy/nancy_all/data.nc11val' ...
using cache file: /tmp/eb02-abc6-04cd-b075
... done.
Loaded fraction:  100%
Sequences:        500
Sequence lengths: 172..3226
Total timesteps:  490430

Creating the neural network... done.
Layers:
(0) input [size: 382]
(1) feedforward_identity [size: 382, bias: 1.0, weights: 146306]
(2) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(3) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(4) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(5) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(6) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(7) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(8) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(9) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(10) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(11) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(12) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(13) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(14) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(15) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(16) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(17) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(18) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(19) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(20) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(21) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(22) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(23) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(24) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(25) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(26) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(27) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(28) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(29) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(30) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(31) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(32) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(33) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(34) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(35) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(36) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(37) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(38) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(39) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(40) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(41) feedforward_tanh [size: 382, bias: 1.0, weights: 146306]
(42) feedforward_identity [size: 382, bias: 1.0, weights: 146306]
(43) feedforward_identity [size: 259, bias: 1.0, weights: 99197]
(44) sse [size: 259]
Total weights: 6244049


Creating the optimizer... done.
Optimizer type: Steepest descent with momentum
Max training epochs:       40
Max epochs until new best: 5
Validation error every:    1
Test error every:          1
Learning rate:             1e-05
Momentum:                  0

Starting training...
Print error per sequence / per timestep
 Epoch | Duration |           Training error  |           Validation error|           Test error      |New best 
-------+----------+---------------------------+---------------------------+---------------------------+---------
     1 | NaN detected. Tune the learning rate please.
Adjust the learning rate to 1.000000e-06Reinitialize the weight
     1 |    526.0 |    112839.266 /   113.107 |    107346.133 /   109.441 |                           |  yes   
     2 |    515.3 |    108202.727 /   108.460 |    105096.609 /   107.147 |                           |  yes   
     3 |    515.5 |    106560.312 /   106.813 |    104040.461 /   106.071 |                           |  yes   
     4 |    515.5 |    105523.125 /   105.774 |    102991.773 /   105.002 |                           |  yes   
     5 |    515.4 |    104802.625 /   105.052 |    102989.211 /   104.999 |                           |  yes   
     6 |    515.5 |    104259.891 /   104.507 |    102182.031 /   104.176 |                           |  yes   
     7 |    515.4 |    103822.125 /   104.069 |    101600.414 /   103.583 |                           |  yes   
     8 |    515.3 |    103477.867 /   103.724 |    101248.531 /   103.224 |                           |  yes   
     9 |    515.2 |    103181.852 /   103.427 |    101032.008 /   103.003 |                           |  yes   
    10 |    515.2 |    102927.500 /   103.172 |    100736.984 /   102.703 |                           |  yes   
    11 |    515.1 |    102701.297 /   102.945 |    100580.867 /   102.544 |                           |  yes   
    12 |    515.2 |    102506.422 /   102.750 |    100370.977 /   102.330 |                           |  yes   
    13 |    515.2 |    102324.914 /   102.568 |    100302.227 /   102.259 |                           |  yes   
    14 |    515.2 |    102167.781 /   102.410 |    100210.852 /   102.166 |                           |  yes   
    15 |    515.2 |    102021.531 /   102.264 |    100113.422 /   102.067 |                           |  yes   
    16 |    515.1 |    101883.375 /   102.125 |    100030.008 /   101.982 |                           |  yes   
    17 |    515.3 |    101756.539 /   101.998 |     99866.906 /   101.816 |                           |  yes   
    18 |    515.2 |    101644.289 /   101.886 |     99774.695 /   101.722 |                           |  yes   
    19 |    515.2 |    101533.172 /   101.774 |     99796.898 /   101.744 |                           |  no    
    20 |    515.3 |    101430.633 /   101.672 |     99632.672 /   101.577 |                           |  yes   
    21 |    515.2 |    101334.789 /   101.575 |     99558.070 /   101.501 |                           |  yes   
    22 |    515.2 |    101251.430 /   101.492 |     99415.742 /   101.356 |                           |  yes   
    23 |    515.2 |    101164.164 /   101.404 |     99395.453 /   101.335 |                           |  yes   
    24 |    515.6 |    101078.273 /   101.318 |     99451.766 /   101.392 |                           |  no    
    25 |    515.4 |    101001.891 /   101.242 |     99297.539 /   101.235 |                           |  yes   
    26 |    515.4 |    100931.414 /   101.171 |     99413.594 /   101.354 |                           |  no    
    27 |    515.6 |    100862.266 /   101.102 |     99349.250 /   101.288 |                           |  no    
    28 |    515.4 |    100791.117 /   101.030 |     99072.055 /   101.005 |                           |  yes   
    29 |    515.3 |    100724.930 /   100.964 |     99234.898 /   101.171 |                           |  no    
    30 |    515.3 |    100663.695 /   100.903 |     99091.148 /   101.025 |                           |  no    
    31 |    515.9 |    100602.758 /   100.842 |     99010.531 /   100.943 |                           |  yes   
    32 |    515.4 |    100547.852 /   100.787 |     98948.516 /   100.879 |                           |  yes   
    33 |    515.3 |    100492.789 /   100.731 |     98965.367 /   100.897 |                           |  no    
    34 |    515.4 |    100432.914 /   100.671 |     98875.109 /   100.805 |                           |  yes   
    35 |    515.5 |    100385.203 /   100.624 |     98917.031 /   100.847 |                           |  no    
    36 |    515.5 |    100334.555 /   100.573 |     98789.344 /   100.717 |                           |  yes   
    37 |    515.4 |    100283.391 /   100.522 |     98861.242 /   100.790 |                           |  no    
    38 |    515.3 |    100233.266 /   100.471 |     98854.414 /   100.783 |                           |  no    
    39 |    515.3 |    100187.992 /   100.426 |     98814.250 /   100.742 |                           |  no    
    40 |    515.4 |    100146.219 /   100.384 |     98808.719 /   100.737 |                           |  no    

Maximum number of training epochs reached. Training stopped.
Lowest validation error: 98789.343750

Storing the trained network in 'trained_network.jsn'... done.
Removing cache file(s) ...
