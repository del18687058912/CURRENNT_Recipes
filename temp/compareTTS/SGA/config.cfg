random_seed          = 3839368179
max_epochs_no_best   = 50
max_epochs           = 20
train                = true
network		     = ./network.jsn



train_file           = /work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_TRAIN/data.nc1,/work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_TRAIN/data.nc2,/work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_TRAIN/data.nc3,/work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_TRAIN/data.nc4,/work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_TRAIN/data.nc5,/work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_TRAIN/data.nc6,/work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_TRAIN/data.nc7
val_file             = /work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_VAL/data.nc1
validate_every       = 1
cache_path           = /tmp/xwtemp
#train_file           = /work/smg/wang/PROJ/WE/DNNAM/DATA/F009/DATA_VAL/data.nc1

weights_dist         = uninorm
weights_normal_sigma = 0.1
weights_normal_mean  = 0

stochastic           = true
input_noise_sigma    = 0.0
shuffle_fractions    = false
shuffle_sequences    = true
parallel_sequences   = 1

momentum	     = 0
autosave 	     = true

Optimizer            = 1
learning_rate        = 0.0001


# Configuration of the MDN
#  these configurations are moved to network.jsn
#  But basically, the mdn_middle.config is the mdn.config for SAR (generator)
#  The mdn.config is the mdn.config for the discriminator
#mdn_config           = ./mdn.config
#mdn_config_middle    = ./mdn_middle.config

# Initialization
#  use the pre-trained SAR as the generator
trainedModel         = /work/smg/wang/PROJ/WE/DNNAM/MODEL/F009/ARRMDN/002/epoch002.autosave
#  only initialize the layers corresponding to the SAR layers
trainedModelCtr      = 0111111000000000000000000000000000


# Configuration for SAR generator 
#  initialization parameters (not used here)
wInitPara            = 20
varInitPara          = 0.001
#  don't tie the varivance across dimensions
tieVariance          = 0
#  AR configuration
mdnDyn               = 01100
#  Stable AR constraint
tanhAutoReg          = 1

# RunningMode
#  Since part of the generator is fixed, there is no need to 
#  do back-propagation to those layers.
#  Another thing is that, this is necessary for SGA 
#  because of a bug in setting the learning_rate for postoutputLayers
runningMode          = 1
